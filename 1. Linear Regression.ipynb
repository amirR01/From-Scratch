{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16adc323",
   "metadata": {},
   "source": [
    "<br><font face=\"Times New Roman\" size=5><div dir=ltr align=center>\n",
    "<font color=blue size=8>\n",
    "    Introduction to Machine Learning <br>\n",
    "<font color=red size=5>\n",
    "    Sharif University of Technology - Computer Engineering Department <br>\n",
    "    Fall 2022<br> <br>\n",
    "<font color=black size=6>\n",
    "    Homework 2: Practical - Linear Regression\n",
    "    </div>\n",
    "<br><br>\n",
    "<font size=4>\n",
    "   **Name**: Amir Hosein Rahmati<br>\n",
    "   **Student ID**: 99103922 <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585264a",
   "metadata": {},
   "source": [
    "<font face=\"Times New Roman\" size=4><div dir=ltr>\n",
    "# Problem 1: Linear Regression Model (40 + 30 optional points)\n",
    "According to <a href=\"https://github.com/asharifiz/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_02_Classical_Models/Linear%20regression.ipynb\"><font face=\"Roboto\">Linear Regression Notebook</font></a>, train a linear regression model on an arbitrary dataset. Explain your chosen dataset and split your data into train and test sets, then predict values for the test set using your trained model. Try to find the best hyperparameters for your model. (Using Lasso Regression, Ridge Regression or Elastic Net and comparing them will have extra optional points)\n",
    "<br> Explain each step of your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20697c04",
   "metadata": {},
   "source": [
    "# importing libraries  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "387d3a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3f7d7",
   "metadata": {},
   "source": [
    "# data\n",
    "the data is about some houses in USA , there are some features about houses in the area that the house is.and also there is the price of house .\n",
    "<br>\n",
    "\n",
    " features : [ AVG Area Income , AVG Area House Age , AVG Area number of rooms , AVG Area Number of Bedrooms , Area Population]\n",
    " </br>\n",
    " <br>\n",
    "<b>the goal is to find a linear regression between features and the price. </b>\n",
    "</br>\n",
    "<br>\n",
    "data link : https://www.kaggle.com/datasets/dmvreddy91/usahousing?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "234a1ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Area Income</th>\n",
       "      <th>Avg. Area House Age</th>\n",
       "      <th>Avg. Area Number of Rooms</th>\n",
       "      <th>Avg. Area Number of Bedrooms</th>\n",
       "      <th>Area Population</th>\n",
       "      <th>Price</th>\n",
       "      <th>Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79545.458574</td>\n",
       "      <td>5.682861</td>\n",
       "      <td>7.009188</td>\n",
       "      <td>4.09</td>\n",
       "      <td>23086.800503</td>\n",
       "      <td>1.059034e+06</td>\n",
       "      <td>208 Michael Ferry Apt. 674\\nLaurabury, NE 3701...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79248.642455</td>\n",
       "      <td>6.002900</td>\n",
       "      <td>6.730821</td>\n",
       "      <td>3.09</td>\n",
       "      <td>40173.072174</td>\n",
       "      <td>1.505891e+06</td>\n",
       "      <td>188 Johnson Views Suite 079\\nLake Kathleen, CA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61287.067179</td>\n",
       "      <td>5.865890</td>\n",
       "      <td>8.512727</td>\n",
       "      <td>5.13</td>\n",
       "      <td>36882.159400</td>\n",
       "      <td>1.058988e+06</td>\n",
       "      <td>9127 Elizabeth Stravenue\\nDanieltown, WI 06482...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63345.240046</td>\n",
       "      <td>7.188236</td>\n",
       "      <td>5.586729</td>\n",
       "      <td>3.26</td>\n",
       "      <td>34310.242831</td>\n",
       "      <td>1.260617e+06</td>\n",
       "      <td>USS Barnett\\nFPO AP 44820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59982.197226</td>\n",
       "      <td>5.040555</td>\n",
       "      <td>7.839388</td>\n",
       "      <td>4.23</td>\n",
       "      <td>26354.109472</td>\n",
       "      <td>6.309435e+05</td>\n",
       "      <td>USNS Raymond\\nFPO AE 09386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Area Income  Avg. Area House Age  Avg. Area Number of Rooms  \\\n",
       "0      79545.458574             5.682861                   7.009188   \n",
       "1      79248.642455             6.002900                   6.730821   \n",
       "2      61287.067179             5.865890                   8.512727   \n",
       "3      63345.240046             7.188236                   5.586729   \n",
       "4      59982.197226             5.040555                   7.839388   \n",
       "\n",
       "   Avg. Area Number of Bedrooms  Area Population         Price  \\\n",
       "0                          4.09     23086.800503  1.059034e+06   \n",
       "1                          3.09     40173.072174  1.505891e+06   \n",
       "2                          5.13     36882.159400  1.058988e+06   \n",
       "3                          3.26     34310.242831  1.260617e+06   \n",
       "4                          4.23     26354.109472  6.309435e+05   \n",
       "\n",
       "                                             Address  \n",
       "0  208 Michael Ferry Apt. 674\\nLaurabury, NE 3701...  \n",
       "1  188 Johnson Views Suite 079\\nLake Kathleen, CA...  \n",
       "2  9127 Elizabeth Stravenue\\nDanieltown, WI 06482...  \n",
       "3                          USS Barnett\\nFPO AP 44820  \n",
       "4                         USNS Raymond\\nFPO AE 09386  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pn.read_csv(\"./USA_Housing.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133fdc9",
   "metadata": {},
   "source": [
    "we will first normalize the features values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "197489b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Area Income</th>\n",
       "      <th>Avg. Area House Age</th>\n",
       "      <th>Avg. Area Number of Rooms</th>\n",
       "      <th>Avg. Area Number of Bedrooms</th>\n",
       "      <th>Area Population</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.686822</td>\n",
       "      <td>0.441986</td>\n",
       "      <td>0.501502</td>\n",
       "      <td>0.464444</td>\n",
       "      <td>0.329942</td>\n",
       "      <td>0.425210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.683521</td>\n",
       "      <td>0.488538</td>\n",
       "      <td>0.464501</td>\n",
       "      <td>0.242222</td>\n",
       "      <td>0.575968</td>\n",
       "      <td>0.607369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.483737</td>\n",
       "      <td>0.468609</td>\n",
       "      <td>0.701350</td>\n",
       "      <td>0.695556</td>\n",
       "      <td>0.528582</td>\n",
       "      <td>0.425192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.506630</td>\n",
       "      <td>0.660956</td>\n",
       "      <td>0.312430</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.491549</td>\n",
       "      <td>0.507384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.469223</td>\n",
       "      <td>0.348556</td>\n",
       "      <td>0.611851</td>\n",
       "      <td>0.495556</td>\n",
       "      <td>0.376988</td>\n",
       "      <td>0.250702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Area Income  Avg. Area House Age  Avg. Area Number of Rooms  \\\n",
       "0          0.686822             0.441986                   0.501502   \n",
       "1          0.683521             0.488538                   0.464501   \n",
       "2          0.483737             0.468609                   0.701350   \n",
       "3          0.506630             0.660956                   0.312430   \n",
       "4          0.469223             0.348556                   0.611851   \n",
       "\n",
       "   Avg. Area Number of Bedrooms  Area Population     Price  \n",
       "0                      0.464444         0.329942  0.425210  \n",
       "1                      0.242222         0.575968  0.607369  \n",
       "2                      0.695556         0.528582  0.425192  \n",
       "3                      0.280000         0.491549  0.507384  \n",
       "4                      0.495556         0.376988  0.250702  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#min-max rescaling \n",
    "data = data.drop(\"Address\",axis = 1)\n",
    "for fe in data.columns :\n",
    "    data[fe] = (data[fe] - min(data[fe])) / (max(data[fe]) - min(data[fe]))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e31627",
   "metadata": {},
   "source": [
    "it's time to separate the train , validation and the test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dbb594a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = data[\"Price\"]\n",
    "data_x = data.drop(\"Price\",axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_x , data_y , test_size=0.20, random_state=42)\n",
    "x_train,x_valid, y_train, y_valid = train_test_split(x_train , y_train, test_size = 0.20,random_state=42)\n",
    "\n",
    "# reshaping data\n",
    "x_train = np.append(np.ones((x_train.shape[0], 1)), x_train , axis=1)\n",
    "x_valid = np.append(np.ones((x_valid.shape[0], 1)), x_valid , axis=1)\n",
    "x_test = np.append(np.ones((x_test.shape[0], 1)), x_test , axis=1)\n",
    "\n",
    "y_train = np.array(y_train).reshape(y_train.shape[0],1)\n",
    "y_valid = np.array(y_valid).reshape(y_valid.shape[0],1)\n",
    "y_test = np.array(y_test).reshape(y_test.shape[0],1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346317a",
   "metadata": {},
   "source": [
    "# linear Regression  \n",
    "$$\n",
    "\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "$$\n",
    "if $\\hat{y}$ be our linear function the loss function will be define as L(w):\n",
    "\n",
    "$$ L(w) = \\frac{1}{N} \\lVert Xw - y \\rVert ^2$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67844e1f",
   "metadata": {},
   "source": [
    "# gradient decent \n",
    "<br>we want to minimize the loss function by optimizing coefficient vector $w$, using gradient decent approach . <br>\n",
    "$$\n",
    "w_{n + 1} := w_n - \\alpha \\frac{\\partial L(w_n)}{\\partial w}\n",
    "$$\n",
    "$ \\alpha $ is the learning rate(lr) , and the gradient of loss function in respect to $ w $ will be :\n",
    "$$\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\frac{2}{N}X^T(Xw - y)\n",
    "$$\n",
    "the gradient decent approach will be executed $ iter $ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "afb41b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_calculator(y,predicted_y):\n",
    "    # print(predicted_y.shape , y.shape)\n",
    "    return predicted_y - y\n",
    "\n",
    "def mse(error,sample_size):\n",
    "    return ((np.linalg.norm(error))**2) / sample_size \n",
    "\n",
    "def predict(X, W):\n",
    "    return np.dot(X, W)\n",
    "\n",
    "def generate_random_coefficients(coefficients_number):\n",
    "    return np.random.rand(coefficients_number,1)\n",
    "\n",
    "def calc_gradient(X, error):\n",
    "    gradient = np.dot(X.T, error)\n",
    "    return gradient\n",
    "\n",
    "def update_weights(W, lr, gradient):\n",
    "    new_weights = W - lr * gradient\n",
    "    return new_weights\n",
    "\n",
    "def train_model_lr_gd(X, Y, W , iter, lr):\n",
    "    losses = []\n",
    "    for i in range(iter):\n",
    "        # generating the predictions \n",
    "        predicted_y = predict(X, W)\n",
    "        #calculating the error\n",
    "        error = error_calculator(Y , predicted_y)\n",
    "        # calculating the loss\n",
    "        loss = mse(error,Y.shape[0])\n",
    "        # adding the loss to our loss list \n",
    "        losses.append(loss)\n",
    "        # calculating gradients\n",
    "        gradient = (calc_gradient(X, error)  / (Y.size))\n",
    "        # updating weights and biases\n",
    "        W = update_weights(W, lr, gradient)\n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf2c43f",
   "metadata": {},
   "source": [
    "# train and validation\n",
    "we will train the model with different hyperparameter $ iter $ and $ \\alpha (lr)$ , then we will choose the best hyperparameter using the validation data.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3828/1824035401.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "  new_weights = W - lr * gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mse on the train data for the chosen model is = 0.0017079164520838064\n",
      "and best mse on the validation data for the chosen model is = 0.0016939979915936022\n",
      "the hyperparameter : iter =5000 , lr =[0.5011003326315726]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alpha_set = np.random.rand(10,1).tolist()\n",
    "iter_set = [500 , 5000 , 10000]\n",
    "initial_w = generate_random_coefficients(x_train.shape[1])\n",
    "\n",
    "best_W = generate_random_coefficients(x_train.shape[1])\n",
    "best_losses =[]\n",
    "best_validation_lost = float(\"inf\")\n",
    "best_iter = 0\n",
    "best_alpha = 0\n",
    "# print(x_train.shape, initial_w.shape , y_train.shape)\n",
    "for iter in iter_set:\n",
    "    for alpha in alpha_set:\n",
    "        # train\n",
    "        W , losses = train_model_lr_gd(x_train,y_train,initial_w,iter,alpha)\n",
    "        # validation \n",
    "        predicted = predict(x_valid,W)\n",
    "        error = error_calculator(y_valid,predicted)\n",
    "        validation_loss = mse(error,y_valid.shape[0])\n",
    "\n",
    "        if (validation_loss < best_validation_lost):\n",
    "            best_W = W\n",
    "            best_losses = losses\n",
    "            best_validation_lost = validation_loss\n",
    "            best_iter = iter\n",
    "            best_alpha = alpha\n",
    "\n",
    "print(f\"best mse on the train data for the chosen model is = {best_losses[-1]}\")\n",
    "print(f\"and best mse on the validation data for the chosen model is = {best_validation_lost}\")\n",
    "print(f\"the hyperparameter : iter ={best_iter} , lr ={best_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbceab2e",
   "metadata": {},
   "source": [
    "# plot \n",
    "u can see the the loss changes by any iteration in plot below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "7546a667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_losses.remove(max(best_losses))\n",
    "fig, axs = plt.subplots(1,1,figsize=(8, 8))\n",
    "axs.plot(np.arange(0, len(best_losses)), best_losses, 'og-')\n",
    "axs.set_xlabel('iteration of gradient descent')\n",
    "axs.set_ylabel('loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf6b05",
   "metadata": {},
   "source": [
    "# test\n",
    "the mse of trained model on the test samples will be calculated in the next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a7667d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mse of test samples by the trained model will be = 0.001680065317324872\n"
     ]
    }
   ],
   "source": [
    "predicted = predict(x_test,best_W)\n",
    "error = error_calculator(y_test,predicted)\n",
    "test_loss = mse(error,y_test.shape[0])\n",
    "\n",
    "print(f\"the mse of test samples by the trained model will be = {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcdcda6",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "$$\n",
    "if $\\hat{y}$ be our linear function the loss function will be define as L(w):\n",
    "\n",
    "$$ L(w) = \\frac{1}{N} ( \\lVert Xw - y \\rVert_2 ^2  + \\lambda \\lVert w \\rVert_1  )$$\n",
    "\n",
    "the gradient of new loss function in respect to $ w $ will be :\n",
    "$$\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\frac{1}{N}(X^T(Xw - y) + \\lambda sign(w))\n",
    "$$\n",
    "the sign function on vector w returns a vector with 1 and -1 values ,for any positive element we will have 1 and for any negative element we will have -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eee429e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_calc_gradient(X, error, w , la):\n",
    "    gradient = np.dot(X.T, error) + (la * np.sign(w))\n",
    "    return gradient\n",
    "def train_lasso_model_lr_gd(X, Y, W , iter, lr , la ):\n",
    "    losses = []\n",
    "    for i in range(iter):\n",
    "        # generating the predictions \n",
    "        predicted_y = predict(X, W)\n",
    "        #calculating the error\n",
    "        error = error_calculator(Y , predicted_y)\n",
    "        # calculating the loss\n",
    "        loss = mse(error,Y.shape[0])\n",
    "        # adding the loss to our loss list \n",
    "        losses.append(loss)\n",
    "        # calculating gradients\n",
    "        gradient = (lasso_calc_gradient(X, error,W,la)  / (Y.size))\n",
    "        # updating weights and biases\n",
    "        W = update_weights(W, lr, gradient)\n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698740c",
   "metadata": {},
   "source": [
    "# train and validation lasso regression\n",
    "we will train the model with different hyperparameter $ iter $ , $ \\alpha (lr)$ ,and $ \\lambda $, then we will choose the best hyperparameter using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8a0ee7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3828/1824035401.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "  new_weights = W - lr * gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mse on the train data for the chosen model is = 0.001708291239630039\n",
      "and best mse on the validation data for the chosen model is = 0.0016927621009720315\n",
      "the hyperparameter : iter =5000 , lr =[0.10998584019624402] , lambda =[0.010156509615643627]\n"
     ]
    }
   ],
   "source": [
    "lambda_set = alpha_set = np.random.rand(5,1).tolist()\n",
    "alpha_set = np.random.rand(5,1).tolist()\n",
    "iter_set = [500 , 5000 , 10000]\n",
    "initial_w = generate_random_coefficients(x_train.shape[1])\n",
    "\n",
    "best_W = generate_random_coefficients(x_train.shape[1])\n",
    "best_losses =[]\n",
    "best_validation_lost = float(\"inf\")\n",
    "best_iter = 0\n",
    "best_alpha = 0\n",
    "best_lambda = 0\n",
    "# print(x_train.shape, initial_w.shape , y_train.shape)\n",
    "for iter in iter_set:\n",
    "    for alpha in alpha_set:\n",
    "        for la in lambda_set:\n",
    "            # train\n",
    "            W , losses = train_lasso_model_lr_gd(x_train,y_train,initial_w,iter,alpha,la)\n",
    "            # validation \n",
    "            predicted = predict(x_valid,W)\n",
    "            error = error_calculator(y_valid,predicted)\n",
    "            validation_loss = mse(error,y_valid.shape[0])\n",
    "\n",
    "            if (validation_loss < best_validation_lost):\n",
    "                best_W = W\n",
    "                best_losses = losses\n",
    "                best_validation_lost = validation_loss\n",
    "                best_iter = iter\n",
    "                best_alpha = alpha\n",
    "                best_lambda = la\n",
    "\n",
    "print(f\"best mse on the train data for the chosen model is = {best_losses[-1]}\")\n",
    "print(f\"and best mse on the validation data for the chosen model is = {best_validation_lost}\")\n",
    "print(f\"the hyperparameter : iter ={best_iter} , lr ={best_alpha} , lambda ={best_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd544528",
   "metadata": {},
   "source": [
    "# test\n",
    "the mse of trained lasso regression model on the test samples will be calculated in the next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b88c3a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mse of test samples by the trained model will be = 0.0016801781875158516\n"
     ]
    }
   ],
   "source": [
    "predicted = predict(x_test,best_W)\n",
    "error = error_calculator(y_test,predicted)\n",
    "test_loss = mse(error,y_test.shape[0])\n",
    "\n",
    "print(f\"the mse of test samples by the trained model will be = {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45653747",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "$$\n",
    "if $\\hat{y}$ be our linear function the loss function will be defined as L(w):\n",
    "\n",
    "$$ L(w) = \\frac{1}{N} ( \\lVert Xw - y \\rVert_2 ^2  + \\lambda \\lVert w \\rVert^2  )$$\n",
    "\n",
    "the gradient of new loss function in respect to $ w $ will be :\n",
    "$$\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\frac{1}{N}(X^T(Xw - y) + \\lambda w)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dae230a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_calc_gradient(X, error, w , la):\n",
    "    gradient = np.dot(X.T, error) + la * w\n",
    "    return gradient\n",
    "def train_ridge_model_lr_gd(X, Y, W , iter, lr , la ):\n",
    "    losses = []\n",
    "    for i in range(iter):\n",
    "        # generating the predictions \n",
    "        predicted_y = predict(X, W)\n",
    "        #calculating the error\n",
    "        error = error_calculator(Y , predicted_y)\n",
    "        # calculating the loss\n",
    "        loss = mse(error,Y.shape[0])\n",
    "        # adding the loss to our loss list \n",
    "        losses.append(loss)\n",
    "        # calculating gradients\n",
    "        gradient = (ridge_calc_gradient(X, error,W,la)  / (Y.size))\n",
    "        # updating weights and biases\n",
    "        W = update_weights(W, lr, gradient)\n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18442868",
   "metadata": {},
   "source": [
    "# train and validation Ridge regression\n",
    "we will train the model with different hyperparameter $ iter $ , $ \\alpha (lr)$ ,and $ \\lambda $, then we will choose the best hyperparameter using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b6a76d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3828/1824035401.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "  new_weights = W - lr * gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mse on the train data for the chosen model is = 0.0017088445830709088\n",
      "and best mse on the validation data for the chosen model is = 0.001691916093729806\n",
      "the hyperparameter : iter =5000 , lr =[0.14676310427096562] , lambda =[0.20269216996824502]\n"
     ]
    }
   ],
   "source": [
    "lambda_set = alpha_set = np.random.rand(5,1).tolist()\n",
    "alpha_set = np.random.rand(5,1).tolist()\n",
    "iter_set = [500 , 5000 , 10000]\n",
    "initial_w = generate_random_coefficients(x_train.shape[1])\n",
    "\n",
    "best_W = generate_random_coefficients(x_train.shape[1])\n",
    "best_losses =[]\n",
    "best_validation_lost = float(\"inf\")\n",
    "best_iter = 0\n",
    "best_alpha = 0\n",
    "best_lambda = 0\n",
    "# print(x_train.shape, initial_w.shape , y_train.shape)\n",
    "for iter in iter_set:\n",
    "    for alpha in alpha_set:\n",
    "        for la in lambda_set:\n",
    "            # train\n",
    "            W , losses = train_ridge_model_lr_gd(x_train,y_train,initial_w,iter,alpha,la)\n",
    "            # validation \n",
    "            predicted = predict(x_valid,W)\n",
    "            error = error_calculator(y_valid,predicted)\n",
    "            validation_loss = mse(error,y_valid.shape[0])\n",
    "\n",
    "            if (validation_loss < best_validation_lost):\n",
    "                best_W = W\n",
    "                best_losses = losses\n",
    "                best_validation_lost = validation_loss\n",
    "                best_iter = iter\n",
    "                best_alpha = alpha\n",
    "                best_lambda = la\n",
    "\n",
    "print(f\"best mse on the train data for the chosen model is = {best_losses[-1]}\")\n",
    "print(f\"and best mse on the validation data for the chosen model is = {best_validation_lost}\")\n",
    "print(f\"the hyperparameter : iter ={best_iter} , lr ={best_alpha} , lambda ={best_lambda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd22bd7",
   "metadata": {},
   "source": [
    "# test\n",
    "the mse of trained ridge regression model on the test samples will be calculated in the next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1deb757b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mse of test samples by the trained model will be = 0.0016799840411246493\n"
     ]
    }
   ],
   "source": [
    "predicted = predict(x_test,best_W)\n",
    "error = error_calculator(y_test,predicted)\n",
    "test_loss = mse(error,y_test.shape[0])\n",
    "\n",
    "print(f\"the mse of test samples by the trained model will be = {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810df44e",
   "metadata": {},
   "source": [
    "# Elastic Net Regression\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "$$\n",
    "if $\\hat{y}$ be our linear function the loss function will be defined as L(w):\n",
    "\n",
    "$$ L(w) = \\frac{1}{N} ( \\lVert Xw - y \\rVert_2 ^2  + \\lambda \\lVert w \\rVert^2 + \\beta \\lVert w \\rVert_1  )$$\n",
    "\n",
    "the gradient of new loss function in respect to $ w $ will be :\n",
    "$$\n",
    "\\frac{\\partial L(w)}{\\partial w} = \\frac{1}{N}(X^T(Xw - y) + \\lambda w + \\beta sign(w))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "72d571df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elastic_net_calc_gradient(X, error, w , la , be):\n",
    "    gradient = np.dot(X.T, error) + (be * np.sign(w)) + (la * w)\n",
    "    return gradient\n",
    "def train_lasso_model_lr_gd(X, Y, W , iter, lr , la , be  ):\n",
    "    losses = []\n",
    "    for i in range(iter):\n",
    "        # generating the predictions \n",
    "        predicted_y = predict(X, W)\n",
    "        #calculating the error\n",
    "        error = error_calculator(Y , predicted_y)\n",
    "        # calculating the loss\n",
    "        loss = mse(error,Y.shape[0])\n",
    "        # adding the loss to our loss list \n",
    "        losses.append(loss)\n",
    "        # calculating gradients\n",
    "        gradient = (Elastic_net_calc_gradient(X, error,W,la,be)  / (Y.size))\n",
    "        # updating weights and biases\n",
    "        W = update_weights(W, lr, gradient)\n",
    "    return W, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b0d6cd",
   "metadata": {},
   "source": [
    "# train and validation Elastic Net  regression\n",
    "we will train the model with different hyperparameter $ iter $ , $ \\alpha (lr)$ , $ \\lambda $,and $ \\beta\\ $ then we will choose the best hyperparameter using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "79becdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3828/1824035401.py:19: RuntimeWarning: invalid value encountered in subtract\n",
      "  new_weights = W - lr * gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mse on the train data for the chosen model is = 0.00171581906704635\n",
      "and best mse on the validation data for the chosen model is = 0.001693019762716589\n",
      "the hyperparameter : iter =5000 , lr =[0.5572886194675679] , lambda =[0.6974451233844517] , beta = [0.2888428685666008]\n"
     ]
    }
   ],
   "source": [
    "lambda_set = alpha_set = np.random.rand(5,1).tolist()\n",
    "Beta_set = alpha_set = np.random.rand(5,1).tolist()\n",
    "alpha_set = np.random.rand(5,1).tolist()\n",
    "iter_set = [500 , 5000]\n",
    "initial_w = generate_random_coefficients(x_train.shape[1])\n",
    "\n",
    "best_W = generate_random_coefficients(x_train.shape[1])\n",
    "best_losses =[]\n",
    "best_validation_lost = float(\"inf\")\n",
    "best_iter = 0\n",
    "best_alpha = 0\n",
    "best_lambda = 0\n",
    "best_beta = 0\n",
    "# print(x_train.shape, initial_w.shape , y_train.shape)\n",
    "for iter in iter_set:\n",
    "    for alpha in alpha_set:\n",
    "        for la in lambda_set:\n",
    "            for be in Beta_set:\n",
    "                # train\n",
    "                W , losses = train_ridge_model_lr_gd(x_train,y_train,initial_w,iter,alpha,la)\n",
    "                # validation \n",
    "                predicted = predict(x_valid,W)\n",
    "                error = error_calculator(y_valid,predicted)\n",
    "                validation_loss = mse(error,y_valid.shape[0])\n",
    "\n",
    "                if (validation_loss < best_validation_lost):\n",
    "                    best_W = W\n",
    "                    best_losses = losses\n",
    "                    best_validation_lost = validation_loss\n",
    "                    best_iter = iter\n",
    "                    best_alpha = alpha\n",
    "                    best_lambda = la\n",
    "                    best_beta = be \n",
    "\n",
    "print(f\"best mse on the train data for the chosen model is = {best_losses[-1]}\")\n",
    "print(f\"and best mse on the validation data for the chosen model is = {best_validation_lost}\")\n",
    "print(f\"the hyperparameter : iter ={best_iter} , lr ={best_alpha} , lambda ={best_lambda} , beta = {best_beta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20524393",
   "metadata": {},
   "source": [
    "# test \n",
    "the mse of trained ridge regression model on the test samples will be calculated in the next part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6e8d3c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mse of test samples by the trained model will be = 0.0016847668408565973\n"
     ]
    }
   ],
   "source": [
    "predicted = predict(x_test,best_W)\n",
    "error = error_calculator(y_test,predicted)\n",
    "test_loss = mse(error,y_test.shape[0])\n",
    "\n",
    "print(f\"the mse of test samples by the trained model will be = {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0ba12c",
   "metadata": {},
   "source": [
    "# conclusion\n",
    "<br> <b> simple regression without regularization term : </b> </br>\n",
    "there is a good linear regression for this data set and as it is showed the mse for the test samples are less than 0.01 \n",
    "<br></br>\n",
    "<br><b> lasso regression: </b> <br>\n",
    "using lasso regression will affect the weights as it wont allow them to get so large.\n",
    "in our model, after validation , our hyperparameter for lasso term has been chosen 0.01 . which is a small number and decreases the effect of regularization term, and our model will be so similar to the regression without regularization term.\n",
    "<br></br>\n",
    "<br><b> Ridge regression: </b> <br>\n",
    "using ridge regression will affect the weights as it wont allow them get so large and wont allow them to have a big standard deviation. we can see the effect of ridge regularization term , the mse on the test samples has been decreased a bit.\n",
    "<br></br>\n",
    "\n",
    "<br><b> Elastic Net regression: </b> <br>\n",
    "we can use both lasso and ridge regularization terms in Elastic Net regression models. as it is showed after validation the hyperparameter of both terms hadn't chosen too large or too small. although there is a small increase in mse of test samples , it is expected that the trained Elastic Net model be a proper model to predict other samples target by their features value.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
